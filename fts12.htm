[dropcap]有[/ dropcap]一天，我的朋友尼尔（Niel）让他在印度的<a href="http://en.wikipedia.org/wiki/Virtual_assistant">虚拟助理</a>给他找一辆自行车，等他到达的那天就可以直接购买了。她给他发送了一个列满来自世界各地自行车出售信息的清单。尼尔说：“不对，我只需要一辆今天就可以<em>在牛津</em>（Oxford）买到的自行车；它必须能在当地买得到。”然后她又给他发送了一长串可以在牛津买到的自行车信息，大部分价格昂贵。于是尼尔澄清说，他想要的是一辆便宜的自行车。之后她又发了一张儿童自行车信息的列表给他。他又澄清说，他需要的是一辆能在当地买到，便宜并适合成年男性骑的自行车。因此她又给他发送了一张清单，列明了在牛津当地的废旧成人自行车。

通常来说，人类对彼此间愿求的理解都会比这个例子好。我们的<a href="http://lesswrong.com/lw/rl/the_psychological_unity_of_humankind/">进化心理统一</a>驱使我们能理解共同的常识及共同的愿求。如果让我给你找一辆自行车，我就会<em>假设</em>你想要的是一辆能满足正常使用、适合你的尺寸，无需用金子打造而成，等等这些条件的车子。——尽管你实际上并未<em>透露</em>任何这类信息。

但如果一个具备不同思想架构，没有与我们一同进化的人，又如何了解我们的共同常识呢。他甚至不知道什么<em>不</em>该做。你会如何做蛋糕呢？“不要用鱿鱼。不要用伽马辐射。也不要用丰田车。”什么<em>不</em>该做这张清单上的名目真是多不胜数啊。

有些人认为，先进的人工智能机器会是某种超级管家，以令人难以置信的<a href="http://facingthesingularity.com/2011/playing-taboo-with-intelligence/">效率</a>执行他们发出的任何指令。其实更准确地来说，应该要将其想象成一个结果气泵：一个没有感情的装置，可以使某些结果具有更大的可能，其他的结果则可能性更少些。（虽然这个结果气泵并<a href="http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/">不具备什么魔法</a>。所以，如果你让它执行一个可能性极低的结果，它就会中断）。

现在，让我们来假设，你的母亲被困在一座着火的大楼里。你正坐在轮椅上，所以你不能直接去救她。但是你却拥有这个<a href="http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/">结果气泵</a>：<blockquote>于是你会大喊“将我母亲从大楼里救出来！”…并按下确认键。

过了一会儿，好像什么都没有发生。你向四周张望，等待消防车架起云梯，救援人员的到达——甚至只是一个强壮、又跑得快的人来将你母亲从大楼里拖出来……

轰隆一声！紧随着雷鸣般的咆哮，在大楼底下的煤气总管发生了爆炸。随着大楼整体结构的分裂，像是慢镜头在播放，你瞥见自己母亲支零破碎的尸体被抛到了空中，快速地炸开去，离之前大楼的中心位置越来越远。</blockquote>
幸运的是，结果气泵有一个反悔按钮，可以将时间倒流。于是你按下按键，再次发指令。“<em>不要炸毁大楼</em>，带我母亲离开那儿，”你说完，马上按下确认键。

然后你母亲就从窗外坠落，并摔断了她的脖子。

在按了十多次的反悔按钮后，你又告诉结果气泵：
<blockquote>在接下来的十分钟里，将我母亲（定义为：跟我有着一半相同基因，及生下我的女人）带离，让她舒服地坐在我身旁的这把椅子上，不受任何身体或精神上的伤害。</blockquote>
于是你看到全体13个消防员在第一时间冲进了房子。他们当中的一个碰巧找到了你母亲，并且很快地将她带到安全的地方。剩余的所有人要么丧生，要么受了重伤。那个消防队员将你母亲安放到椅子上，然后转身回到他那些丧生或受重伤的同事们的身边。虽然你的愿望是达成了，但是却没有得到你<em>想要的</em>结果。

问题是，对于你想要与不想要的结果，你的大脑还没有大到足以包含了注明<em>每个可能性细节</em>的指令。你怎么会知道你既希望自己母亲能毫发无损地逃离大楼，同时又<em>不要</em>让一打的消防员丧生或致残呢？这可不是因为你的大脑遍布了“我希望我母亲能毫发无损地逃离大楼，同时不要让一打的消防员丧生或致残”的指令。相反地，当你<em>看到</em>自己母亲得以安全逃离大楼的时候，却牺牲了一打的消防员，你才意识到“哦，该死的。<em>那</em>不是我想要的。”或许，你早就可能已经<em>想像</em>到那具体的情景，并意识到“哦，不，那并不是我想要的。”可是，在事情发生之前，或在你想像当时的情景之前，在你的大脑里却遍寻不着那么具体的信息。那是不可能的，因为你的大脑根本没有这样的空间啊。

但是你承担不起如此大的代价，你不能愣坐在那里，将结果气泵握在手里，一边想像着数百万种可能发生的结果，一边留意哪些是你想要的，哪些是不想要的。<a href="http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/">当你还在思前想后的时候</a>，你母亲早已命送黄泉了。
<blockquote>要是她的头被压碎了，只留下身体呢？要是她的身体被压碎，只剩下头呢？如果有一支人体冷冻队伍在外等候，准备做头部冷冻呢？一个冷冻的头代表一个人吗？特里·夏沃（Terry Schiavo）是一个人吗？一只黑猩猩会值多少钱？</blockquote>
始终，你的大脑还没达到<em>极其</em>复杂的程度。有部分有限的指令合集，可以描述出那个让你做出判断的系统。如果我们能理解大脑里的每个神经元、神经递质与蛋白质是如何运作的，并且拥有一张大脑的完整地图，那么一个人工智能机器至少能<em>在原则上</em>计算出，对于一个有限的可能性结果合集，你会做出怎样的判断。

这里要阐明的寓意是<a href="http://lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/">并没有比整个人类价值体系还小的安全愿望</a>:
<blockquote>有太多可能穿越时间的路径了。你难以想象出你给【结果气泵】发出指令后，能通往目的地的所有的路径来。“如果要将你母亲与大楼中心之间的距离最大化”，更有效的方法莫过于引爆核武器了……或者利用【结果气泵】的更高水平智力，做一些你我都想不到的事情来，就像一只黑猩猩不会想到引爆核武器这个点子一样。你不可能想象出所有穿越时间的路径来，比你对一个象棋机进行编程，通过对每个可能的棋盘里的每一步棋进行硬编码还要多的路径。

而真正的生活要比国际象棋复杂得多。你不能提前预知，提前用你哪个需要的价值观来判断出，【结果气泵】用于穿越时间的路径。特别是，除了从着火的大楼拯救出你母亲之外，如果你还有更长期或更大范围的愿望。

…唯一安全的【人工智能只不过是一台机器】做法是，对其分享你所有的判断标准，在这一点上，你只要说出“我想要你执行我该要做的事情。”</blockquote>
有一群来自家庭作坊的人提出了一个简单的原理，说可以让人工智能机器执行我们想要做的。<a href="http://lesswrong.com/lw/lp/fake_fake_utility_functions/">却没有一个会行得通</a>。我们的行为不仅仅是为了 <a href="http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/">幸福</a>或<a href="http://lesswrong.com/lw/65w/not_for_the_sake_of_pleasure_alone/">快乐</a>。<a href="http://lesswrong.com/lw/y3/value_is_fragile/">我们所评价的是高度复杂的东西</a>。进化为你带来的是<a href="http://lesswrong.com/lw/l3/thou_art_godshatter/">欲望的千块碎片</a>。（如需了解它是如何让你的神经生物学变成一团糟的话，可以阅读<em><a href="http://www.amazon.com/Neuroscience-Preference-Choice-Cognitive-Mechanisms/dp/0123814316/">神经科学的偏好和选择</a></em>一书的前两章。）

这也是为什么道德哲学家们花上几千年的时间，还<em>未能</em>找到一套简单、一旦颁布就能创造出一个我们渴求世界的准则。每当有人提出一小套的道德准则，<a href="http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf">其他人总是能找出些漏洞来</a>。酌情地删减，甚至一些看起来微不足道，<a href="http://lesswrong.com/lw/y3/value_is_fragile/">却可以引发致命错误的漏洞</a>：
<blockquote>细想下作为极为重要的人类价值之一的“烦恼”——我们希望不要一次又一次地“重蹈覆辙”。你可以想像一种思想，囊括了几乎人类价值的整体规范，几乎所有的道德与元道德，可是<em>唯独遗忘了这件事</em>——

——所以它耗尽了一切，直到时间的尽头，直到光锥的最远点，反反复复地回放着单次高度优化的体验。

或者想像一种思想，几乎囊括了人类最喜欢的感觉类型的整体规范——但并不意味着，那些感觉都有着重要的<em>外部参照对象</em>。这样的话，你的想法只能围绕着<em>感觉</em>打转，就像找到了一项重大发现一样，<em>感觉</em>它已经找到了完美的爱人，<em>感觉</em>它帮助了一个朋友，但其实并没<em>真正在做</em>那些事情，却已经沦为了自己的<a href="http://en.wikipedia.org/wiki/Experience_machine">体验机</a>。并且一旦思想追寻那些感觉和参照对象，它就将成为美好的未来与真理；但却由于这一价值的<em>单面度</em>被忽视了，未来也就成为了枯燥的东西。无聊却不断重复着，因为虽然这种思想<em>觉得</em>它正适逢难以置信的新奇体验，而这种感觉已经不再明智与真实。

或者来看一个相反的问题：一个机器具备了人类所有的价值，<em>除了</em>主观经验的估值之外。因此，得出的结果是，一个没有感情的优化器，绕着制造真实的发现而转，然而这些发现却不见得令人享受与喜爱，因为那里根本没有人这么做……

价值不仅是复杂的，它还是<em>脆弱</em>的。人类的价值绝非单面的，如果这样的话，那么只要失去了一件东西，未来就会变得毫无价值了。只需吹一口气，就能粉碎掉所有的价值。并不是所有价值都如此的不堪一击——但如果使用超过“一口气”的功力，或许能办到。</blockquote>
你应该可以预见事情将会如何进展。因为我们从来没有对整个人类价值体系进行过解码，我们不知道应该如何为人工智能机器制定价值。我们不知道该抱什么希望。如果将来我们创造出超级人工智能机器，我们<em>只能</em>给它一个<em>可悲而不完整的</em>价值体系，然后它将继续做那些我们不愿发生的事情，因为它将会按照我们所<em>希望的</em>去做，而不是我们<em>想要的</em>。

现在，我们只知道如何建造出能够进行优化处理，却<em>并非</em>获得我们想要结果的人工智能机器。我们只知道如何建造出危险的人工智能机器。更糟糕的是，当我们研究如何使人工智能机器变得<em>安全</em>时，要远远落后于研究如何使其<em>强大</em>的速度，因为我们将更多的资源分配到人工智能机器的功能问题上，这要比我们对人工智能的安全性问题投入得多。

时间在流逝。<a href="http://facingthesingularity.com/2011/superstition-in-retreat/">人工智能即将到来</a>。可我们还没准备好。
