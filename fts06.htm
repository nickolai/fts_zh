[dropcap]在[/dropcap]<em>终结者（The Terminator）</em>电影中，<a href="http://en.wikipedia.org/wiki/Skynet_(Terminator)">天网（Skynet）</a>人工智能变得有自我意识，杀死了数十亿的人们，还派出机器人杀手来歼灭剩余的一群人类抵抗战士。这听起来很糟糕，但在<em>国家公共广播电台（NPR）</em>的<a href="http://www.npr.org/2011/01/11/132840775/The-Singularity-Humanitys-Last-Invention">关于智力爆炸</a>节目中，易趣网（Ebay）的程序员基夫·罗特沙默（Keefe Roedersheimer）却解释道，创造一个<em>真实的</em>机器超智能会比那更<em>糟糕</em>。

“会比<em>终结者（The Terminator）</em>电影中的还要糟糕吗？NPR的主持人马丁·卡斯特（Martin Kaste）问道。

“对，比那还要糟糕几倍，”基夫回答。

“在电影里面那简直就是一片废墟啊，人们藏匿在被烧毁的建筑物中，躲闪着激光的扫射。还有什么能比<em>那种情景</em>更糟糕的呀？”

“所有的人都死了。”

他为什么会这么说呢？为了确保人工智能的大多数目标——无论是解决 <a href="http://singinst.org/ourresearch/publications/CFAI/info/glossary.html#gloss_riemann_hypothesis_catastrophe">黎曼假设（Riemann hypothesis）</a>还是将石油的产量最大化——原因很简单，<a href="http://singinst.org/upload/artificial-intelligence-risk.pdf">那就是</a>“人工智能并不爱你，它也不恨你，但由于你是由原子组成的，对它来说还会有其它用途。”而当一个超人类人工智能注意到我们人类很可能会<em>抗拒</em>将我们的原子用作“其它用途”，进而<em>威胁</em>到人工智能的利益与它的目标时，将会激发它想<em>尽快</em>彻底地摧毁人类 ——它当然不希望让一支勇猛、可以拯救世界的英雄队伍了解其致命的弱点，除非是他们能放下自身的分歧……不可能。为了确保人工智能的大多数目标，尽可能高效地消除人类对其目标的威胁，才能实现人工智能的<a href="http://facingthesingularity.com/2011/the-laws-of-thought/">预期效用</a>最大化。

<a href="http://facingthesingularity.com/wp-content/uploads/2011/12/mushroomcloud.jpg"><img class="alignright size-medium wp-image-134" title="mushroomcloud" src="http://facingthesingularity.com/wp-content/uploads/2011/12/mushroomcloud-300x223.jpg" alt="" width="300" height="223" /></a>让我们面对现实吧：其实有更多轻易就能杀死我们人类的方法，要比派出<em>走</em>在<em>大街</em>上，似乎很享受将人类扔到墙上，而不是折断他们脖子的<em>人形</em>机器人要简单得多。甚至还有更好的方法，如果采取突然攻击人类的办法，在世界各地引发同步效应，公然向存在的反抗势力宣战，将会来得更加迅速与致命。例如：为什么不运用到超智能的一切能力设计一种能在空气中传播、具高传染性及致命的超级病毒呢？一场<a href="http://en.wikipedia.org/wiki/1918_flu_pandemic">1918年的变异流感</a>席卷了地球上3%的人口，而那甚至是发生在空气传播（在全球范围内传播疾病）变得普及之前，可是却没有一丝一毫的智能进入到病毒的设计。只需运用一点小聪明，你就能从荷兰找到一支队伍，制造出一个变种禽流感，“<a href="http://rt.com/news/bird-flu-killer-strain-119/?utm_medium=referral&amp;utm_source=pulsenews">便能消灭掉一半的人类</a>。”一个超智能甚至还可以制造出远比这更糟的后果。或许人工智能还能通过隐藏在地下或将其自身在太空引爆，运用<em>现有的</em>技术：<a href="http://lesswrong.com/lw/8f0/existential_risk/">数千核武器</a>，就能杀死我们所有人了。

关键不在于这两种<em>特定</em>场景都很可能。我只是想指出在现实中会出现的情况，当然，这完全与怎样才能让故事更加吸引人无关。正如牛津大学的哲学家尼克·博斯特罗姆（Nick Bostrom）<a href="http://www.nickbostrom.com/existential/risks.html">如此表述</a>：
<blockquote>你最后一次看到有关人类突然进入灭绝的电影是什么时候呢（毫无预警地，并且未被其他文明形态取代）？</blockquote>
故事会变得更加有趣，假如任何一方在战争中当真能获胜。如果弗罗多（Frodo）做了明智的抉择，从巨鹰的背后<a href="http://www.youtube.com/watch?v=1yqVD0swvWU">把戒指扔到火山里</a>，<em>魔戒</em>一书就不会如此畅销了。如果人类突然就<em>失败</em>，结局的话，这就不会成为一个有趣的故事了。

当思考人工智能的时候，我们一定不能<a href="http://lesswrong.com/lw/k9/the_logical_fallacy_of_generalization_from/">从虚构的证据中来归纳</a>。不幸的是，<em>我们的大脑会自动这样处理。</em>

<em></em>一项著名的<a href="http://commonsenseatheism.com/wp-content/uploads/2011/12/Lichtenstein-et-al-Judged-frequency-of-lethal-events.pdf">1978年研究</a>让受试者判断有哪两种危险发生得更频繁一些。受试者们认为意外事故会像疾病一样造成众多人员死亡，以及凶杀要比自杀案件更加频繁。事实上，由疾病导致的死亡人数是事故死亡人数的16倍，自杀案件更是普通凶杀案件的两倍。这究竟是怎么回事呢？

数十个针对<a href="http://wiki.lesswrong.com/wiki/Availability_heuristic">可得性启发式</a>的研究提出，我们是通过事情<em>有多轻易在脑海中浮现</em>来判断事情的频率或机率的。当我们不能在维基百科中查出<em>实际的</em>频率，或运用<a href="http://yudkowsky.net/rational/bayes">贝叶斯定理</a>来确定实际的可能性时，就会运用到从我们祖辈的生活中进化而来的一种启发式，那情况还不算太糟糕。大脑的启发式是快速、廉价的，而且通常都是对的。

但是就正如我们许多的进化认知启发式一样，可得性启发式常常会得出错误的结果。意外事故要比疾病来得更加逼真，因此更容易让人们联想到，导致我们高估了它们相对于疾病的发生频率。同样地也错误评估了凶杀与自杀案例的频率。

可得性启发式同时还<a href="http://www.psychologytoday.com/articles/200712/10-ways-we-get-the-odds-wrong">解释</a>了为什么人们认为坐飞机要比驾车更危险，而事实恰恰相反：然而一架飞机坠毁的情景更生动，并且一旦飞机失事就会广泛地报道，所以在人们的印象中它的发生机率更高，而大脑诱使自身联想到意外的<em>可发生性</em>以指示其<em>发生机率</em>。

当你的大脑想到超人类人工智能的时候，它究竟在做什么呢？最有可能的是，它会检索所有你曾遇到过的超人类人工智能的实例——因为它们都是<em>虚构的</em>——而根据它们与最容易联想到的情景的吻合程度（由于你的思维只在虚幻小说中碰到过它们）来判断某些情况的可能性，这样做只会让事情变得更糟。<a href="http://lesswrong.com/lw/k9/the_logical_fallacy_of_generalization_from/">换句话来说</a>：“就是联想到突然冒进脑海的小说情景，并代替你进行思考。”

所以如果你产生了有关超人类人工智能形象的直觉，这些直觉可能只是源自于<em>虚幻小说</em>，而你并不会察觉到。

这就是为什么我要通过谈论<a href="http://facingthesingularity.com/2011/from-skepticism-to-technical-rationality/">理性</a>开始切入<em>面对奇点</em>。在我们开始思考人工智能的那一刻，也是我们直接闯入一个错综复杂但普遍的人为故障模式的时候。收集虚构的证据用于归纳就是其中一种模式。这里再举一些<a href="http://singinst.org/upload/cognitive-biases.pdf">其它例子</a>：
<ul>
	<li>基于可得性启发式的缘故，你的大脑会告诉你人工智能消灭人类是如此令人难以置信，也是不可能的，因为你以前从未遇到过这种情况。此外，即使当事情随着让人精疲力竭而变得糟糕，比方说，<a href="http://tvtropes.org/pmwiki/pmwiki.php/Main/AlienInvasion">外星人入侵</a>这部电影，勇敢的人类英雄最终总是会在在最后一分钟找到获取胜利的方法。</li>
	<li>因为我们高估了连环事件发生的机率，但低估了间隔事件发生的机率（<a href="http://fiesta.bren.ucsb.edu/~costello/courses/ESM204/ESM204_10/Readings/Judgment%20under%20uncertainty.pdf">这里有一份研究</a>），我们很可能<em>高估</em>了超人类人工智能结果会变好的这一机率，因为X，Y和Z全部都会发生，而我们可能<em>低估</em>了超人类人工智能最后变得更糟的机率，因为有许多方法会让超人类人工智能最后变得更糟，这并不依赖于还有多少其它事件会发生。</li>
	<li>基于你大脑的<a href="http://lesswrong.com/lw/j7/anchoring_and_adjustment/">锚定与调整</a>启发式的缘故，你对一种情况的判断将会<em>锚定</em>明显无关的信息。（于是这个出现在幸运陀螺滚动下的数字将会影响你对有多少个国家在非洲的猜测）。尽管我刚刚<em>谈到</em>对于判断超人类人工智能最终会如何，<em>终结者（The Terminator）</em>只是无关紧要的虚构证据，但你的大脑却试图锚定<em>终结者（The Terminator）</em>，然后调整远离它，但却还远得不够离开它。</li>
	<li>基于<a href="http://lesswrong.com/lw/lg/the_affect_heuristic/">情感启发式</a>的缘故，我们会根据自己对事物的感觉来对其做判断。我们对漂亮的人感觉良好，所以我们就会认为漂亮的人同样也会是聪明和勤奋的。我们对智能感觉良好，因此我们可能就会期望一个智能机器是仁慈的。（但是，<a href="http://singinst.org/upload/ECAPFoxShulman.pdf">结果并非如此</a>。）
	<li>基于<a href="http://lesswrong.com/lw/hw/scope_insensitivity/">范围低敏度</a>的缘故，相比我们努力阻止1000万人死亡，我们不会感觉到更大的动力去阻止100亿人死亡。</li>
</ul>
很显然地，我们并不为思考人工智能这一问题而生于世上。

为了能明智地思考关于人工智能问题，我们必须要提防——并且积极地抵制——许多种常见的思维误区。在思考人工智能问题时，我们将不得不运用到<a href="http://facingthesingularity.com/2011/the-laws-of-thought/">思维的定律</a>而不是<a href="http://facingthesingularity.com/2011/the-crazy-robots-rebellion/">普通的人类疯狂思维</a>。

或者，换一种方式思考：超人类人工智能将会对我们的世界产生重大的冲击，所以我们迫切地希望“战胜”超人类人工智能，而不是被其“打败”。<a href="http://facingthesingularity.com/2011/from-skepticism-to-technical-rationality/">技术合理性</a> <a href="http://facingthesingularity.com/2011/why-spock-is-not-rational/">使用得当</a>的话会是<a href="http://lesswrong.com/lw/7i/rationality_is_systematized_winning/">一个最佳的获胜系统</a>——事实上，这是<a href="http://selfawaresystems.files.wordpress.com/2012/03/rational_ai_greater_good.pdf">一个完美的人工智能会为了尽最大可能获胜而使用的系统</a>。所以，如果我们想要战胜超人类人工智能，我们就应该用理性来解决。
而那仅仅是我们要在下一章里开始做的。

