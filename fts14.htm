<img class="alignright size-full wp-image-285" title="rocking stone atop mountain" src="http://facingthesingularity.com/wp-content/uploads/2012/01/rocking-stone-atop-mountain.jpg" alt="" width="300" height="210" />[dropcap]我[/dropcap]们在地球历史上的关键时刻发现了自己。像一块栖息在山脉顶峰的巨石般，我们立于险峻的巅峰。可是我们不能一直停驻在原点上：只要科学不断进步，<a href="http://facingthesingularity.com/2011/superstition-in-retreat/">人工智能就会到来</a>。很快我们就要从山脉的一侧倒塌，或坠落到另一侧的稳定休憩点上。

第一种情况就是人类灭绝。（“灭绝？就呆在那个角落吧。”）而另一个休憩点可能是<a href="http://books.google.com/books?id=X5jdMyJKNL4C&amp;lpg=PP1&amp;dq=global%20catastrophic%20risks&amp;pg=PT822#v=onepage&amp;q&amp;f=false">坚定的全球性极权主义</a>，阻止了科学的进步，虽然那听起来似乎不太靠谱。

那么人工智能又会怎样呢？人工智能会引发一场<a href="http://facingthesingularity.com/2012/intelligence-explosion/">智能大爆炸</a>，然而，正是由于我们不懂得如何给人工智能机器设定<a href="http://facingthesingularity.com/2012/value-is-complex-and-fragile/">仁慈的目标</a>，因此在默认的情况下，智能大爆炸将会以<a href="http://facingthesingularity.com/2012/value-is-complex-and-fragile/">意外的灾难性</a>结局来优化这个世界。一场 <em>受控的</em>智能大爆炸，另一方面可以优化世界，让它变得更加美好。（在接下来的帖子，我们将讨论更多与这一选项相关的内容。）

就我自己来说，正将<em>我自身</em>所有的重量都倾 靠在这座最后的峡谷方向上：一场受控的智能大爆炸。

在这个稍纵即逝的历史时刻，我们能够理解（尽管朦胧）自身的现状，并改变我们将可能坠落到山脉的哪一侧。那又如何，接下来，我们会付诸行动吗？
<h3>微分知识发展</h3>
我们需要的是<a href="http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf">微分知识发展</a>：
<blockquote>微分知识发展主要在于，处理<em>增加</em>风险的知识发展前，优先处理<em>降低</em>风险的知识发展。特别是应用到人工智能风险方面，微分知识发展计划会对我们提出忠告，在有关人工智能<em>安全</em>的哲学、科学与技术问题方面的发展，已经超出了人工智能的<em>性能</em>发展，以致我们要在开发出专制的超级人工智能机器前，率先开发出<em>安全的</em>超级人工智能机器。我们的第一个超级人工智能机器必须是安全的，因为我们可能再不会有机会重来一次。</blockquote>
<img class="alignright size-full wp-image-318" title="Two-Horses" src="http://facingthesingularity.com/wp-content/uploads/2012/02/Two-Horses.jpg" alt="" width="226" height="170" />粗略来说，人工智能的安全研究正在与人工智能的性能研究进行赛跑。目前，人工智能的性能研究占了上风，它确实是领先了一步。因为人类正努力研究人工智能的性能，却忽略了安全方面的研究。

如果人工智能的性能在这场竞赛中跑赢了，那么人类就输了。相反，如果人工智能安全研究赢了，人类也会赢。

很多人都明白，推动人工智能的性能研究会有什么后果。那也是你能在人工智能相关的<em>大部分</em>作品中所读到的。反之，如果我们推动人工智能的安全研究，又会怎样呢？

<a href="http://lukeprog.com/SaveTheWorld.html">这篇文章</a> 罗列了一连串有关人工智能安全研究的问题类别，现在，还是让我先举几个例子吧。（如果你想避开这些可怕的技术术语，可以跳过这个列表）。　　
<ul>
	<li>当一台机器认为对自身决策机制的修改过于激进，它又如何能确保这样做能增加其期望效用呢？当前的决策理论就在这一点上，将<a href="http://www.youtube.com/watch?v=MwriJqBZyoM">Löb's定理绊倒</a>了，所以需要一种全新的“反映一致的”决策理论。</li>
	<li>一台机器的实用功能可以指代其状态，或当中的实体，其技术。但正如彼得·德·布兰科（Peter de Blanc）<a href="http://arxiv.org/pdf/1105.3821v1.pdf">指出</a>，“如果机器能够升级或更改本质 ，它将会面临危机：机器原本的【实用功能】就可能会因为其新的本质而无法明确定义。“如何才能确保，当我们为人工智能机器订下好的目标后，那些目标不会在机器进行本质更新时遭到“毁坏”呢，对此我们需要想出解决办法。</li>
	<li>我们要如何从人类“希望”的角度出发，解释令人满意的实用功能呢？当前在人工智能机器中的<a href="http://lesswrong.com/r/discussion/lw/a73/a_brief_tutorial_on_preferences_in_ai/">偏好获取方式</a>是不够的：我们需要更新、<a href="http://lesswrong.com/lw/9jh/the_humans_hidden_utility_function_maybe/">更强大的</a>方式，以及偏好获取的通用算法。又或许我们应该让人类自己花一段很长的时间，去推算出他们的偏好，直到他们<a href="http://singinst.org/upload/CEV.html">在自身的偏好中找到“反思平衡”</a>。后者可能涉及到<a href="http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf">模拟整个大脑</a>——但如果没有首先研发出能启发大脑、自我完善却又危险的人工智能机器，我们又怎能构造得出来呢？</li>
	<li>在人工智能机器被研发出来之前，我们未必能解决好价值理论的问题。也许与之相反，我们需要的是一套关于如何处理这种规范不确定性的理论，例如：像博斯特罗姆（Bostrom）所提出的<a href="http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html">议会模型</a>。</li>
</ul>
除了这些“技术”研究问题外，我们也可以考虑用微分知识发展，针对各种“战略”研究问题的发展而提出建议。人类应该将资金投向或抽离哪项技术呢？我们要怎么做才能减低人工智能军备竞赛的风险呢？它会降低人工智能的风险，从而激励广泛的<a href="http://lesswrong.com/lw/9hb/position_design_and_write_rationality_curriculum/">理性培训</a>或<a href="http://lesswrong.com/lw/7nl/moral_enhancement/">仁爱培训</a>吗？我们应该考虑首先进行哪项干预措施呢？
<h3>马上，行动</h3>
所以，人工智能风险问题的解决方案之一就是：微分知识发展。解决方案之二是：根据我们能做出的最佳策略研究的建议来<em>采取行动</em>。例如：以下的行动可能会减少人工智能的风险：
<ul>
	<li>向正在我们上述讨论的人工智能安全领域，进行着各种技术与策略类研究的机构<strong>捐资</strong>——就像<a href="http://singinst.org/donate/">奇点研究所</a>和<a href="http://www.fhi.ox.ac.uk/donate">人类未来研究所</a>这类型的机构。</li>
	<li><strong>说服人们认真看待人工智能的安全</strong>，尤其是人工智能研究人员、慈善家、聪明的年轻人，以及位高权重的人。</li>
</ul>
<div>在此页面<a href="http://lesswrong.com/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/">（如何有效降低人工智能的风险）</a>中，将针对有效降低人工智能风险，可以实施的具体项目提供更多详情，并附带了每个项目估计要花费的成本。</div>
<h3>机遇</h3>
至今为止我一直在谈人工智能的<em>风险</em>，但<a href="http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf">也不要</a>忽略掉人工智能带来的<em>机遇</em>：
<blockquote>通常我们不会将癌症治疗或经济稳定性与人工智能联想到一起，但是癌症治疗终究是个需要足够聪明才智，才能找出治愈办法的问题，而实现经济稳定终究也是个需要足够聪明才智，才能找出实现办法的问题。无论我们的目标设定在哪一程度之上，只要充分地运用好人工智能，我们将可以在更大程度上实现目标。</blockquote>
在我的最后一张帖子里，我会尝试解释，只要我们能<em>下定决心采取行动</em>并<em><strong>好好</strong>利用人工智能</em>，事情将会变得<em>美好</em>。

没错，<a href="http://facingthesingularity.com/2012/no-god-to-save-us/">在物理学上，即使是非常<em>糟糕</em>的后果，也没有任何事情能阻挡其发生</a>，对于这一事实，我们必须保持清醒。但是，我们也必须认楚这一事实，对于比我们原始的猴脑所能想象到的，更大的快乐与和谐这样的结果，在物理学上同样无法阻挡。
